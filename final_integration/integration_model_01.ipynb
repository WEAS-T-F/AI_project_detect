{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\ai_project_for_jupyter\\final_integration\\Model.py:9: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os, sys\n",
    "import itertools, time\n",
    "from Model import get_Model\n",
    "from parameter import letters\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0-rc0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Settings ( do not run in no GPU env )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "start_time = time.time()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    #print(img.shape)\n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n",
    "    \"\"\"\n",
    "    NOTE: this is the function you might want to use as a starting point once you want to \n",
    "    average/extrapolate the line segments you detect to map out the full\n",
    "    extent of the lane (going from the result shown in raw-lines-example.mp4\n",
    "    to that shown in P1_example.mp4).  \n",
    "    \n",
    "    Think about things like separating line segments by their \n",
    "    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n",
    "    line vs. the right line.  Then, you can average the position of each of \n",
    "    the lines and extrapolate to the top and bottom of the lane.\n",
    "    \n",
    "    This function draws `lines` with `color` and `thickness`.    \n",
    "    Lines are drawn on the image inplace (mutates the image).\n",
    "    If you want to make the lines semi-transparent, think about combining\n",
    "    this function with the weighted_img() function below\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "def draw_circle(img,lines, color=[0, 0, 255]):\n",
    "    for line in lines:\n",
    "        cv2.circle(img,(line[0],line[1]), 2, color, -1)\n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_arr = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    #draw_lines(line_arr, lines)\n",
    "    return lines\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + λ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)\n",
    "\n",
    "def Collect_points(lines):\n",
    "\n",
    "    # reshape [:4] to [:2]\n",
    "    interp = lines.reshape(lines.shape[0]*2,2)\n",
    "    # interpolation & collecting points for RANSAC\n",
    "    for line in lines:\n",
    "        if np.abs(line[3]-line[1]) > 5:\n",
    "            tmp = np.abs(line[3]-line[1])\n",
    "            a = line[0] ; b = line[1] ; c = line[2] ; d = line[3]\n",
    "            slope = (line[2]-line[0])/(line[3]-line[1]) \n",
    "            for m in range(0,tmp,5):\n",
    "                if slope>0:\n",
    "                    new_point = np.array([[int(a+m*slope),int(b+m)]])\n",
    "                    interp = np.concatenate((interp,new_point),axis = 0)\n",
    "                elif slope<0:\n",
    "                    new_point = np.array([[int(a-m*slope),int(b-m)]])\n",
    "                    interp = np.concatenate((interp,new_point),axis = 0)                \n",
    "    return interp\n",
    "\n",
    "def get_random_samples(lines):\n",
    "    one = random.choice(lines)\n",
    "    two = random.choice(lines)\n",
    "    if(two[0]==one[0]): # extract again if values are overlapped\n",
    "        while two[0]==one[0]:\n",
    "            two = random.choice(lines)\n",
    "    one, two = one.reshape(1,2), two.reshape(1,2)\n",
    "    three = np.concatenate((one,two),axis=1)\n",
    "    three = three.squeeze()\n",
    "    return three\n",
    "\n",
    "def compute_model_parameter(line):\n",
    "    # y = mx+n\n",
    "    m = (line[3] - line[1])/(line[2] - line[0])\n",
    "    n = line[1] - m*line[0]\n",
    "    # ax+by+c = 0\n",
    "    a, b, c = m, -1, n\n",
    "    par = np.array([a,b,c])\n",
    "    return par\n",
    "\n",
    "def compute_distance(par, point):\n",
    "    # distance between line & point\n",
    "    \n",
    "    return np.abs(par[0]*point[:,0]+par[1]*point[:,1]+par[2])/np.sqrt(par[0]**2+par[1]**2)\n",
    "\n",
    "def model_verification(par, lines):\n",
    "    # calculate distance\n",
    "    distance = compute_distance(par,lines)\n",
    "    # total sum of distance between random line and sample points    \n",
    "    sum_dist = distance.sum(axis=0)\n",
    "    # average\n",
    "    avg_dist = sum_dist/len(lines)\n",
    "    \n",
    "    return avg_dist\n",
    "\n",
    "def draw_extrapolate_line(img, par,color=(0,0,255), thickness = 2):\n",
    "\n",
    "    x1, y1 = int(-par[1]/par[0]*img.shape[0]-par[2]/par[0]), int(img.shape[0])\n",
    "    x2, y2 = int(-par[1]/par[0]*(img.shape[0]/2+100)-par[2]/par[0]), int(img.shape[0]/2+100)\n",
    "    cv2.line(img, (x1 , y1), (x2, y2), color, thickness)\n",
    "    return img\n",
    "\n",
    "def get_fitline(img, f_lines):\n",
    "\n",
    "    rows,cols = img.shape[:2]\n",
    "    output = cv2.fitLine(f_lines,cv2.DIST_L2,0, 0.01, 0.01)\n",
    "    vx, vy, x, y = output[0], output[1], output[2], output[3]\n",
    "    x1, y1 = int(((img.shape[0]-1)-y)/vy*vx + x) , img.shape[0]-1\n",
    "    x2, y2 = int(((img.shape[0]/2+100)-y)/vy*vx + x) , int(img.shape[0]/2+100)\n",
    "    result = [x1,y1,x2,y2]\n",
    "\n",
    "    return result\n",
    "\n",
    "def draw_fitline(img, result_l,result_r, color=(255,0,255), thickness = 10):\n",
    "    # draw fitting line\n",
    "    lane = np.zeros_like(img)\n",
    "    cv2.line(lane, (int(result_l[0]) , int(result_l[1])), (int(result_l[2]), int(result_l[3])), color, thickness)\n",
    "    cv2.line(lane, (int(result_r[0]) , int(result_r[1])), (int(result_r[2]), int(result_r[3])), color, thickness)\n",
    "    # add original image & extracted lane lines\n",
    "    final = weighted_img(lane, img, 1,0.5)  \n",
    "    return final\n",
    "\n",
    "def erase_outliers(par, lines):\n",
    "    # distance between best line and sample points\n",
    "    distance = compute_distance(par,lines)\n",
    "\n",
    "    #filtered_dist = distance[distance<15]\n",
    "    filtered_lines = lines[distance<13,:]\n",
    "    return filtered_lines\n",
    "\n",
    "def smoothing(lines, pre_frame):\n",
    "    # collect frames & print average line\n",
    "    lines = np.squeeze(lines)\n",
    "    avg_line = np.array([0,0,0,0])\n",
    "    \n",
    "    for ii,line in enumerate(reversed(lines)):\n",
    "        if ii == pre_frame:\n",
    "            break\n",
    "        avg_line += line\n",
    "    avg_line = avg_line / pre_frame\n",
    "\n",
    "    return avg_line\n",
    "\n",
    "def ransac_line_fitting(img, lines, min=100):\n",
    "    global fit_result, l_fit_result, r_fit_result\n",
    "    best_line = np.array([0,0,0])\n",
    "    if(len(lines)!=0):                \n",
    "        for i in range(30):           \n",
    "            sample = get_random_samples(lines)\n",
    "            parameter = compute_model_parameter(sample)\n",
    "            cost = model_verification(parameter, lines)                        \n",
    "            if cost < min: # update best_line\n",
    "                min = cost\n",
    "                best_line = parameter\n",
    "            if min < 3: break\n",
    "        # erase outliers based on best line\n",
    "        filtered_lines = erase_outliers(best_line, lines)\n",
    "        fit_result = get_fitline(img, filtered_lines)\n",
    "    else:\n",
    "        if (fit_result[3]-fit_result[1])/(fit_result[2]-fit_result[0]) < 0:\n",
    "            l_fit_result = fit_result\n",
    "            return l_fit_result\n",
    "        else:\n",
    "            r_fit_result = fit_result\n",
    "            return r_fit_result\n",
    "        \n",
    "    try:\n",
    "        if (fit_result[3]-fit_result[1])/(fit_result[2]-fit_result[0]) < 0:\n",
    "            l_fit_result = fit_result\n",
    "            return l_fit_result\n",
    "        else:\n",
    "            r_fit_result = fit_result\n",
    "            return r_fit_result\n",
    "    except:\n",
    "        r_fit_result = fit_result\n",
    "        return r_fit_result\n",
    "\n",
    "def perspective_transform(img):\n",
    "    # [x,y] 좌표점을 4x2의 행렬로 작성\n",
    "    # 좌표점은 좌상->좌하->우상->우하\n",
    "    pts1 = np.float32([[0,0],[350,200],[200,400],[350,400]])\n",
    "\n",
    "    # 좌표의 이동점\n",
    "    pts2 = np.float32([[0,0],[600,0],[0,800],[600,800]])\n",
    "\n",
    "    # pts1의 좌표에 표시. perspective 변환 후 이동 점 확인.\n",
    "    cv2.circle(img, (504,1003), 20, (255,0,0),-1)\n",
    "    cv2.circle(img, (243,1524), 20, (0,255,0),-1)\n",
    "    cv2.circle(img, (1000,1000), 20, (0,0,255),-1)\n",
    "    cv2.circle(img, (1280,1685), 20, (0,0,0),-1)\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "\n",
    "    dst = cv2.warpPerspective(img, M, (1100,1100))\n",
    "\n",
    "    #plt.subplot(121),plt.imshow(img),plt.title('image')\n",
    "    #plt.subplot(122),plt.imshow(dst),plt.title('Perspective')\n",
    "    #plt.show()\n",
    "    return dst\n",
    "    \n",
    "def detect_lanes_img(img):\n",
    "\n",
    "    height, width = img.shape[:2]\n",
    "    pers_img = perspective_transform(img)\n",
    "\n",
    "    # Set ROI\n",
    "    vertices = np.array([[(50,height),(width/2-45, height/2+60), (width/2+45, height/2+60), (width-50,height)]], dtype=np.int32)\n",
    "    ROI_img = region_of_interest(pers_img, vertices)\n",
    "    \n",
    "    # Convert to grayimage\n",
    "    #g_img = grayscale(img)\n",
    "       \n",
    "    # Apply gaussian filter\n",
    "    blur_img = gaussian_blur(img, 3)\n",
    "        \n",
    "    # Apply Canny edge transform\n",
    "    canny_img = canny(blur_img, 70, 210)\n",
    "    # to except contours of ROI image\n",
    "    vertices2 = np.array([[(52,height),(width/2-43, height/2+62), (width/2+43, height/2+62), (width-52,height)]], dtype=np.int32)\n",
    "    canny_img = region_of_interest(canny_img, vertices2)\n",
    "\n",
    "    # Perform hough transform\n",
    "    # Get first candidates for real lane lines  \n",
    "    line_arr = hough_lines(canny_img, 1, 1 * np.pi/180, 30, 10, 20)\n",
    "    \n",
    "    #draw_lines(img, line_arr, thickness=2)\n",
    "\n",
    "    line_arr = np.squeeze(line_arr)\n",
    "    \n",
    "    try:\n",
    "        # Get slope degree to separate 2 group (+ slope , - slope)\n",
    "        slope_degree = (np.arctan2(line_arr[:,1] - line_arr[:,3], line_arr[:,0] - line_arr[:,2]) * 180) / np.pi\n",
    "\n",
    "        # ignore horizontal slope lines\n",
    "        line_arr = line_arr[np.abs(slope_degree)<160]\n",
    "        slope_degree = slope_degree[np.abs(slope_degree)<160]\n",
    "        # ignore vertical slope lines\n",
    "        line_arr = line_arr[np.abs(slope_degree)>95]\n",
    "        slope_degree = slope_degree[np.abs(slope_degree)>95]\n",
    "        L_lines, R_lines = line_arr[(slope_degree>0),:], line_arr[(slope_degree<0),:]\n",
    "        #print(line_arr.shape,'  ',L_lines.shape,'  ',R_lines.shape)\n",
    "    \n",
    "        # interpolation & collecting points for RANSAC\n",
    "        L_interp = Collect_points(L_lines)\n",
    "        R_interp = Collect_points(R_lines)\n",
    "\n",
    "        #draw_circle(img,L_interp,(255,255,0))\n",
    "        #draw_circle(img,R_interp,(0,255,255))\n",
    "\n",
    "        # erase outliers based on best line\n",
    "        left_fit_line = ransac_line_fitting(img, L_interp)\n",
    "        right_fit_line = ransac_line_fitting(img, R_interp)\n",
    "\n",
    "        # smoothing by using previous frames\n",
    "        L_lane.append(left_fit_line), R_lane.append(right_fit_line)\n",
    "\n",
    "        if len(L_lane) > 10:\n",
    "            left_fit_line = smoothing(L_lane, 10)    \n",
    "        if len(R_lane) > 10:\n",
    "            right_fit_line = smoothing(R_lane, 10)\n",
    "        final = draw_fitline(img, left_fit_line, right_fit_line)\n",
    "    except:\n",
    "        final = img\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plate Detection by RCNN Prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Previous weight data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "''' plate Prediction '''\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "Region = {\"A\": \"서울 \", \"B\": \"경기 \", \"C\": \"인천 \", \"D\": \"강원 \", \"E\": \"충남 \", \"F\": \"대전 \",\n",
    "          \"G\": \"충북 \", \"H\": \"부산 \", \"I\": \"울산 \", \"J\": \"대구 \", \"K\": \"경북 \", \"L\": \"경남 \",\n",
    "          \"M\": \"전남 \", \"N\": \"광주 \", \"O\": \"전북 \", \"P\": \"제주 \"}\n",
    "Hangul = {\"dk\": \"아\", \"dj\": \"어\", \"dh\": \"오\", \"dn\": \"우\", \"qk\": \"바\", \"qj\": \"버\", \"qh\": \"보\", \"qn\": \"부\",\n",
    "          \"ek\": \"다\", \"ej\": \"더\", \"eh\": \"도\", \"en\": \"두\", \"rk\": \"가\", \"rj\": \"거\", \"rh\": \"고\", \"rn\": \"구\",\n",
    "          \"wk\": \"자\", \"wj\": \"저\", \"wh\": \"조\", \"wn\": \"주\", \"ak\": \"마\", \"aj\": \"머\", \"ah\": \"모\", \"an\": \"무\",\n",
    "          \"sk\": \"나\", \"sj\": \"너\", \"sh\": \"노\", \"sn\": \"누\", \"fk\": \"라\", \"fj\": \"러\", \"fh\": \"로\", \"fn\": \"루\",\n",
    "          \"tk\": \"사\", \"tj\": \"서\", \"th\": \"소\", \"tn\": \"수\", \"gj\": \"허\"}\n",
    "\n",
    "def decode_label(out):\n",
    "    # out : (1, 32, 42)\n",
    "    out_best = list(np.argmax(out[0, 2:], axis=1))  # get max index -> len = 32\n",
    "    out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
    "    outstr = ''\n",
    "    for i in out_best:\n",
    "        if i < len(letters):\n",
    "            outstr += letters[i]\n",
    "            '''\n",
    "            if letters[i] != \"X\":\n",
    "                outstr += letters[i]\n",
    "            '''\n",
    "    return outstr\n",
    "\n",
    "\n",
    "def label_to_hangul(label):  # eng -> hangul\n",
    "    #print(\"label in prediction:\",label)\n",
    "    \n",
    "    if len(label)==10 and label[9] == \"X\":\n",
    "        region = label[0]\n",
    "        two_num = label[1:3]\n",
    "        hangul = label[3:5]\n",
    "        four_num = label[5:9]\n",
    "        try:\n",
    "            region = Region[region] if region != 'Z' else ''\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            hangul = Hangul[hangul]\n",
    "        except:\n",
    "            pass\n",
    "        return region + two_num + hangul + four_num\n",
    "    else:\n",
    "        #print(\"label:\",label)\n",
    "        region = label[0]\n",
    "        three_num = label[1:4]\n",
    "        hangul = label[4:6]\n",
    "        four_num = label[6:]\n",
    "        if len(four_num)==5:\n",
    "            four_num = label[6:10]\n",
    "        try:\n",
    "            region = Region[region] if region != 'Z' else ''\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            hangul = Hangul[hangul]\n",
    "        except:\n",
    "            pass\n",
    "        return region + three_num + hangul + four_num\n",
    "\n",
    "model = get_Model(training=False)\n",
    "\n",
    "try:\n",
    "    model.load_weights(\"LSTM+BN9--90--3.020.hdf5\")\n",
    "    print(\"...Previous weight data...\")\n",
    "except:\n",
    "    raise Exception(\"No weight file!\")\n",
    "\n",
    "total = 0\n",
    "acc = 0\n",
    "letter_total = 0\n",
    "letter_acc = 0\n",
    "start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the CV2 Windows (실행부분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프레임 너비: 640, 프레임 높이: 480, 초당 프레임 수: 60\n",
      "프레임 너비: 1280, 프레임 높이: 720, 초당 프레임 수: 60\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) # 웹캠 가져오기\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) # 또는 cap.get(3)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # 또는 cap.get(4)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 또는 cap.get(5)\n",
    "print('프레임 너비: %d, 프레임 높이: %d, 초당 프레임 수: %d' %(width, height, fps))\n",
    "cap.set(3,1280)\n",
    "cap.set(4,1024)\n",
    "\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) # 또는 cap.get(3)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # 또는 cap.get(4)\n",
    "print('프레임 너비: %d, 프레임 높이: %d, 초당 프레임 수: %d' %(width, height, fps))\n",
    "\n",
    "fit_result, l_fit_result, r_fit_result, L_lane, R_lane = [], [], [], [], []\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Be sure to use the lower case\n",
    "#out = cv2.VideoWriter('output.mp4', fourcc, 20.0, ( 960, 540 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capture open?: True\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "print(\"capture open?:\",cap.isOpened())\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret: # ret = true or False\n",
    "        result = detect_lanes_img(frame)\n",
    "        #rint(\"loaded line detection model\")\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # 웹캠 이미지 grayscale 변환\n",
    "        \n",
    "        frame2 = frame\n",
    "        img_pred = frame2.astype(np.float32)\n",
    "        img_pred = cv2.resize(img_pred, (128, 64))\n",
    "        img_pred = (img_pred / 255.0) * 2.0 - 1.0\n",
    "        img_pred = img_pred.T\n",
    "        img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "        img_pred = np.expand_dims(img_pred, axis=0)\n",
    "        \n",
    "        net_out_value = model.predict(img_pred)\n",
    "        #rint(\"Predicting license plate\")\n",
    "        pred_texts = decode_label(net_out_value)\n",
    "        #print(\"Predicted texts:\", pred_texts)\n",
    "        cv2.rectangle(result, (0,0), (150, 30), (0,0,0), -1)\n",
    "        cv2.putText(result, pred_texts, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255),2)\n",
    "        \n",
    "        \n",
    "        cv2.imshow('result',result)\n",
    "    else:\n",
    "        break\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
